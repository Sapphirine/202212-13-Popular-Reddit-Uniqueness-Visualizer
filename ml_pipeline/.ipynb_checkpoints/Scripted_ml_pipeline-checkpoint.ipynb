{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alirahman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/12 00:42:40 WARN Utils: Your hostname, ali-PC resolves to a loopback address: 127.0.1.1; using 192.168.50.30 instead (on interface eno2)\n",
      "22/12/12 00:42:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/12 00:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/12 00:42:46 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/12 00:42:46 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# https://stackoverflow.com/questions/51390676/how-to-visualize-pyspark-mls-lda-or-other-clustering\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyspark import SparkConf, SparkContext,SQLContext\n",
    "from pyspark.sql import SparkSession, functions\n",
    "from pyspark.ml.feature import Word2Vec,CountVectorizer,Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.sql.functions import col, udf, countDistinct, regexp_replace\n",
    "from pyspark.sql.types import IntegerType,ArrayType,StringType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit\n",
    "import csv\n",
    "\n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark SQL basic example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "#month = \"November\"\n",
    "years = [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"]\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        csv_path = \"/\" + year + \"/\" + month + \".csv\"\n",
    "        data_path = \"../data\" + csv_path # Data path for csv file\n",
    "        spark_df = spark.read.csv(data_path, inferSchema = True, header=True) # checking the csv file\n",
    "        spark_df = spark_df.withColumn('Title', regexp_replace('Title', '\"', ''))\n",
    "        spark_df = spark_df.withColumn('Title', regexp_replace('Title', '.', ''))\n",
    "        # Topic Modelling on Title (Potentially do it on description if possible)\n",
    "        node = \"Title\"\n",
    "        # Get title data, filter out empty nodes\n",
    "        title_data = spark_df.select(node).filter(functions.col(node).isNotNull())\n",
    "\n",
    "        df2 = spark_df.select(countDistinct(\"Subreddit\"))\n",
    "        topic_num = df2.first()[0]\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"Title\", outputCol=\"words\")\n",
    "        tokenized = tokenizer.transform(spark_df)\n",
    "        remover = StopWordsRemover(stopWords=stopwords.words('english'), inputCol=\"words\", outputCol=\"filtered\")\n",
    "        result = remover.transform(tokenized)\n",
    "        # result.select(\"filtered\").show()\n",
    "\n",
    "        cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "        cvModel = cv.fit(result)\n",
    "        cvResult = cvModel.transform(result)\n",
    "\n",
    "        lda = LDA(maxIter=20, k = 10)\n",
    "        ldaModel = lda.fit(cvResult)\n",
    "        transformed = ldaModel.transform(cvResult).select(\"topicDistribution\")\n",
    "        #transformed.show(truncate=False)\n",
    "\n",
    "        vocab = cvModel.vocabulary\n",
    "        topics = ldaModel.describeTopics()\n",
    "        topics_rdd = topics.rdd\n",
    "\n",
    "        topics_words = topics_rdd\\\n",
    "               .map(lambda row: row['termIndices'])\\\n",
    "               .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "               .collect()\n",
    "        topic_weights = topics_rdd\\\n",
    "               .map(lambda row: row['termWeights'])\\\n",
    "               .collect()\n",
    "\n",
    "        file_path = \"../processed_data\" + csv_path\n",
    "        if not os.path.exists(\"../processed_data/\" + year):\n",
    "            os.makedirs(\"../processed_data/\" + year)\n",
    "        with open(file_path, 'w') as file:\n",
    "            header = [\"term\", \"probability\", \"topic\"]\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "            for idx, topic in enumerate(topics_words):\n",
    "                i = 0\n",
    "                for word in topic:\n",
    "                    data = [idx, word, topic_weights[idx][i]]\n",
    "                    writer.writerow(data)\n",
    "                    i = i+1\n",
    "\n",
    "        dist = ldaModel.transform(cvResult)\n",
    "\n",
    "        ith = udf(ith_, DoubleType())\n",
    "        df = dist.select([\"Title\"] + [ith(\"topicDistribution\", lit(i)).alias('topic_'+str(i)) for i in range(10)] )\n",
    "\n",
    "        df_p = dist.select('topicDistribution').toPandas()\n",
    "        df_p1 = df_p.topicDistribution.apply(lambda x:np.array(x))\n",
    "        df_p2 = pd.DataFrame(df_p1.tolist()).apply(lambda x:x.argmax(),axis=1)\n",
    "        df_p3 = df_p2.reset_index()\n",
    "        df_p3.columns = ['doc','topic']\n",
    "        df2_p = dist.select('Title').toPandas()\n",
    "        #print(df_p3)\n",
    "        final_df = pd.concat([df2_p, df_p3], axis=1)\n",
    "        topic_path = \"../document_topics\" + csv_path\n",
    "        if not os.path.exists(\"../document_topics/\" + year):\n",
    "            os.makedirs(\"../document_topics/\" + year)\n",
    "        final_df.to_csv(topic_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
