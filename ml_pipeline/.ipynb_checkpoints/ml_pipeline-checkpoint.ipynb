{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alirahman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# https://stackoverflow.com/questions/51390676/how-to-visualize-pyspark-mls-lda-or-other-clustering\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyspark import SparkConf, SparkContext,SQLContext\n",
    "from pyspark.sql import SparkSession, functions\n",
    "from pyspark.ml.feature import Word2Vec,CountVectorizer,Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.sql.functions import col, udf, countDistinct, regexp_replace\n",
    "from pyspark.sql.types import IntegerType,ArrayType,StringType\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark SQL basic example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "month = \"December\"\n",
    "year = \"2010\"\n",
    "csv_path = \"/\" + year + \"/\" + month + \".csv\"\n",
    "\n",
    "data_path = \"../data\" + csv_path # Data path for csv file\n",
    "\n",
    "spark_df = spark.read.csv(data_path, inferSchema = True, header=True) # checking the csv file\n",
    "spark_df = spark_df.withColumn('Title', regexp_replace('Title', '\"', ''))\n",
    "# Topic Modelling on Title (Potentially do it on description if possible)\n",
    "node = \"Title\"\n",
    "# Get title data, filter out empty nodes\n",
    "title_data = spark_df.select(node).filter(functions.col(node).isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark_df.select(countDistinct(\"Subreddit\"))\n",
    "topic_num = df2.first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Title\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(spark_df)\n",
    "remover = StopWordsRemover(stopWords=stopwords.words('english'), inputCol=\"words\", outputCol=\"filtered\")\n",
    "result = remover.transform(tokenized)\n",
    "# result.select(\"filtered\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "cvModel = cv.fit(result)\n",
    "cvResult = cvModel.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(maxIter=20, k = 10)\n",
    "ldaModel = lda.fit(cvResult)\n",
    "transformed = ldaModel.transform(cvResult).select(\"topicDistribution\")\n",
    "#transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab = cvModel.vocabulary\n",
    "topics = ldaModel.describeTopics()\n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "topic_weights = topics_rdd\\\n",
    "       .map(lambda row: row['termWeights'])\\\n",
    "       .collect()\n",
    "\n",
    "import csv\n",
    "file_path = \"../processed_data\" + csv_path\n",
    "with open(file_path, 'w') as file:\n",
    "    header = [\"term\", \"probability\", \"topic\"]\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    for idx, topic in enumerate(topics_words):\n",
    "        i = 0\n",
    "        for word in topic:\n",
    "            data = [idx, word, topic_weights[idx][i]]\n",
    "            writer.writerow(data)\n",
    "            i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit\n",
    "dist = ldaModel.transform(cvResult)\n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "ith = udf(ith_, DoubleType())\n",
    "df = dist.select([\"Title\"] + [ith(\"topicDistribution\", lit(i)).alias('topic_'+str(i)) for i in range(10)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/12 00:30:43 WARN DAGScheduler: Broadcasting large task binary with size 1701.6 KiB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_p = dist.select('topicDistribution').toPandas()\n",
    "df_p1 = df_p.topicDistribution.apply(lambda x:np.array(x))\n",
    "df_p2 = pd.DataFrame(df_p1.tolist()).apply(lambda x:x.argmax(),axis=1)\n",
    "df_p3 = df_p2.reset_index()\n",
    "df_p3.columns = ['doc','topic']\n",
    "df2_p = dist.select('Title').toPandas()\n",
    "#print(df_p3)\n",
    "final_df = pd.concat([df2_p, df_p3], axis=1)\n",
    "topic_path = \"../document_topics\" + csv_path\n",
    "final_df.to_csv(topic_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
